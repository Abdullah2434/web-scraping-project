# Web Scraping Project: Keyword Trend Analysis

A beginner-friendly Python project that scrapes trending data based on keywords from Google Trends and Reddit, then visualizes the results in a Streamlit dashboard.

## ğŸ¯ Features

- **Google Trends Integration**: Fetch trending searches and related queries
- **Reddit Data Collection**: Get top posts and discussions for keywords
- **Data Cleaning**: Remove duplicates, normalize text, and process data
- **Database Storage**: Store data in MongoDB or PostgreSQL
- **Interactive Dashboard**: Streamlit app with charts and search functionality

## ğŸ“ Project Structure

```
web-scraping-project/
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ config.py                # Configuration settings
â”œâ”€â”€ fetch_google_data.py     # Google Trends data collection
â”œâ”€â”€ fetch_reddit_data.py     # Reddit data collection
â”œâ”€â”€ clean_data.py            # Data cleaning and processing
â”œâ”€â”€ save_to_db.py            # Database operations
â”œâ”€â”€ app.py                   # Streamlit dashboard
â””â”€â”€ README.md                # This file
```

## ğŸš€ Setup Instructions

### 1. Install Python Dependencies

```bash
pip install -r requirements.txt
```

### 2. Reddit API Setup (Required for Reddit data)

1. Go to [Reddit Apps](https://www.reddit.com/prefs/apps)
2. Click "Create App" or "Create Another App"
3. Fill in the form:
   - **Name**: Your app name (e.g., "Keyword Scraper")
   - **App type**: Select "script"
   - **Description**: Optional description
   - **About URL**: Leave blank or add your website
   - **Redirect URI**: Use `http://localhost:8080`
4. Click "Create app"
5. Note down the **Client ID** (under the app name) and **Client Secret**

### 3. Configure Environment Variables

Create a `.env` file in the project root:

```env
# Reddit API Credentials
REDDIT_CLIENT_ID=your_client_id_here
REDDIT_CLIENT_SECRET=your_client_secret_here
REDDIT_USER_AGENT=keyword_scraper_bot_1.0
REDDIT_USERNAME=your_reddit_username
REDDIT_PASSWORD=your_reddit_password
```

### 4. Database Setup (Choose One)

#### Option A: MongoDB (Recommended for beginners)
- Install [MongoDB Community Edition](https://docs.mongodb.com/manual/installation/)
- Default connection: `mongodb://localhost:27017/`

#### Option B: PostgreSQL
- Install [PostgreSQL](https://www.postgresql.org/download/)
- Create a database named `keyword_trends`
- Update connection details in `config.py`

### 5. Run the Application

#### Collect Data
```bash
# Fetch Google Trends data
python fetch_google_data.py

# Fetch Reddit data
python fetch_reddit_data.py

# Clean and process data
python clean_data.py

# Save to database
python save_to_db.py
```

#### Launch Dashboard
```bash
streamlit run app.py
```

## ğŸ“Š Usage

1. **Data Collection**: Run the fetch scripts with your keywords
2. **Dashboard**: Open the Streamlit app and explore:
   - Enter new keywords to search
   - View keyword frequency charts
   - Browse related queries from Google Trends
   - See Reddit post titles and discussions

## ğŸ”§ Configuration

Edit `config.py` to customize:
- Keywords to search
- Database connection settings
- API rate limits
- Data collection parameters

## ğŸ“š Learning Resources

- [Python Web Scraping Tutorial](https://realpython.com/beautiful-soup-web-scraper-python/)
- [Streamlit Documentation](https://docs.streamlit.io/)
- [Pandas Data Cleaning](https://pandas.pydata.org/docs/user_guide/index.html)
- [MongoDB Python Tutorial](https://pymongo.readthedocs.io/en/stable/tutorial.html)

## âš ï¸ Important Notes

- **Rate Limiting**: Be respectful with API calls to avoid being blocked
- **Reddit API**: Requires free registration and API credentials
- **Data Privacy**: Follow platform terms of service
- **Error Handling**: Check logs if data collection fails

## ğŸ› Troubleshooting

### Common Issues

1. **Reddit API Errors**: Check your `.env` file credentials
2. **Database Connection**: Ensure MongoDB/PostgreSQL is running
3. **Missing Data**: Run data collection scripts before dashboard
4. **Streamlit Errors**: Check all dependencies are installed

### Getting Help

- Check error messages in terminal
- Verify all dependencies are installed: `pip list`
- Ensure database is running and accessible
- Review configuration in `config.py`

## ğŸ“ Next Steps

Once you're comfortable with the basics:
- Add more data sources (Twitter, YouTube, News APIs)
- Implement real-time data updates
- Add user authentication
- Deploy to cloud platforms
- Create automated data collection schedules

Happy scraping! ğŸš€ 